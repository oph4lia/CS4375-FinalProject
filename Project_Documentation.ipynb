{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653944c2",
   "metadata": {},
   "source": [
    "# CS 4375 Project - Training an RL Agent to play the offline Google Dino Game\n",
    "> Mikkael Dumancas (mxd220018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e101a",
   "metadata": {},
   "source": [
    "## Briefing\n",
    "\n",
    "I chose to do option 3 for the project and centered around Reinforcement Learning. At a high level, reinforcement learning is a machine learning technique that uses 4 key concepts:\n",
    "- Action\n",
    "- Reward\n",
    "- Environment\n",
    "- Agent\n",
    "\n",
    "The RL model, or agent, interacts with an environment with the sole goal of taking actions to maximize a reward value. At runtime, this boils down to a simple loop of:\n",
    "\n",
    "1. The agent makes an observation about the environment\n",
    "2. The agent selects an action\n",
    "3. The environment uses this action to determine the reward and the next observation for the agent\n",
    "4. The cycle (1-3) continues until the environment ends/terminates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c534137",
   "metadata": {},
   "source": [
    "## Downloading Dependencies\n",
    "\n",
    "This project is coded in Python 3 using the PyTorch framework. The relevant dependencies used for this project are as follows:\n",
    "\n",
    "- Stable-baselines3: A PyTorch-based library that is used to implement Reinforcement Learning algorithms (for this project, I use DQN)\n",
    "- Gymnasium: A Python library/toolkit developed by OpenAI to handle testing RL algorithms. I utilize its standard API and environment building to handle agent-environment interactions.\n",
    "- MSS: A simple Python module that takes a screenshot of a screen. This is just a lightweight, faster way to grab observations compared to OpenCV.\n",
    "- Pydirectinput: This python library enables direct interaction with the mouse and keyboard, so it makes actions that the RL agent takes very fast.\n",
    "- Numpy: A python library that makes array/matrix handling a lot easier. This helps with processing images (like screenshots from MSS).\n",
    "- OpenCV: This is a computer vision library with useful tools to preprocess the screen captures from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecbcb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install stable-baselines3[extra] protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc57667",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install mss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c40642",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pydirectinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dbe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2e44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import pydirectinput\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import keyboard\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4aaaec",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Building the environment using Gymnasium is just like setting up a class. The primary methods that are needed to build a Gymnasium environment are the constructor(__init__) method and the step method. The constructor sets up the environment and observation shapes. The step method is essentially one iteration of the loop previously mentioned. A step, when called, will simulate an observation, an agent action, and the environment response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a09fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(Env):\n",
    "    # set up environment and observation shapes\n",
    "    def __init__(self):\n",
    "        # subclass the model\n",
    "        super().__init__()\n",
    "        # set up spaces (gymnasium containers to represent an observation and actions)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,83,150), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3) # define 3 unique actions agent can take\n",
    "        # set up parameters for game extracting an observation of the game using mss\n",
    "        self.cap = mss()\n",
    "        # defining observation of environment window size (preprocessing)\n",
    "        self.game_location = {'top':150, 'left':80, 'width':600, 'height':200}\n",
    "        self.done_location = {'top':185, 'left':330, 'width':300, 'height':70}\n",
    "\n",
    "    # simulate agent observation, action, and environment response\n",
    "    def step(self, action):\n",
    "        # agent has 3 actions - 0 = jump (space), 1 = crouch (down key), 2 = nothing\n",
    "        action_map = {\n",
    "            0: 'space',\n",
    "            1: 'down',\n",
    "            2: 'no_op'\n",
    "        }\n",
    "        # use pydirectinput to press key related to desired action\n",
    "        if action != 2:\n",
    "            pydirectinput.press(action_map[action])\n",
    "        \n",
    "        # Gymnasium defines 4 outputs for step: next_observation, reward, terminate, info\n",
    "\n",
    "        # check if game is done/environment is terminated\n",
    "        done, done_cap = self.get_done()\n",
    "        # check if game is done IF TIME CONSTRAINT IS ADDED TO EPISODE\n",
    "        truncated = False\n",
    "        # get the next observation of the environment to return to the agent\n",
    "        next_observation = self.get_observation()\n",
    "        # give the agent a reward of 1 for each frame it is alive\n",
    "        reward = 1 \n",
    "        info = {} # can leave info empty, not entirely necessary for this project\n",
    "        return next_observation, reward, done, truncated, info\n",
    "    \n",
    "    # visualize the game (not really used since I implement game capture via mss)\n",
    "    def render(self):\n",
    "        cv2.imshow('Game,', np.array(self.cap.grab(self.game_location))[:,:,:3])\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "\n",
    "    # ends an observation\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # reset the game environment (can follow environment termination)\n",
    "    def reset(self, *, seed=None, option=None):\n",
    "        # set seed for reproducibility of the reset\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        #set buffer between transitions\n",
    "        # time.sleep(1.5)\n",
    "        #move mouse to top left of screen and click to restart game\n",
    "        pydirectinput.click(x=300, y=300)\n",
    "        pydirectinput.press('space')\n",
    "        \n",
    "        # Gymnasium requires an observation and info to be returned\n",
    "        info = {}\n",
    "        return self.get_observation(), info\n",
    "\n",
    "    # extra methods not required to build Gymnasium environment\n",
    "\n",
    "    # grab observation of the game environment\n",
    "    def get_observation(self):\n",
    "        # grab raw screen capture of the game_location using mss and turning it into an array\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        # preprocessing\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (150,83))\n",
    "        channel = np.reshape(resized, (1,83,150))\n",
    "        return channel\n",
    "    # extract game over text with OCR via pytesseract\n",
    "    def get_done(self):\n",
    "        done = False\n",
    "        # grab raw screen capture of the done_location using mss\n",
    "        done_cap = np.array(self.cap.grab(self.done_location))[:,:,:3]\n",
    "        # coordinates for checking if a certain pixel of game over exists\n",
    "        check_x, check_y = 38, 200\n",
    "        pixel_color = done_cap[check_x, check_y]\n",
    "        # specific rgb value that is expected to appear when game is over\n",
    "        game_over_rgb = np.array([172,172,172])\n",
    "        tolerance = 10\n",
    "        if np.all(np.abs(pixel_color - game_over_rgb) <= tolerance):\n",
    "            done = True\n",
    "        \n",
    "        return done, done_cap\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf43ca",
   "metadata": {},
   "source": [
    "## Test Environment\n",
    "\n",
    "Now that the environment has been built, it is fairly simple to test out the environment. Create a new instance of the game called env, and from there run the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec04da",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae78ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.get_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(obs[0],  cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "done, done_cap = env.get_done()\n",
    "plt.imshow(done_cap)\n",
    "print(done)\n",
    "print(done_cap[38,93])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ccff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347dc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea31070",
   "metadata": {},
   "source": [
    "After testing all the methods, here is a sample run of a training step for the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ac52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(3):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        obs, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(f'Total Reward for episode {episode} is {total_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75391e38",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The RL algorithm I am going to be using is a deep Q-network, also known as DQN. It is a similar network topology to a standard neural network, except it is based on Q-learning. Q-learning is essentially the same as what is described in the briefing. It is centered around an agent moving through different states through actions and receiving rewards until the episode terminates. Mapping this to the gymnasiunm lingo, we can see that:\n",
    "\n",
    "- Q-values are rewards\n",
    "- States are different observations of the environment\n",
    "- Agent actions and rewards are functionally the same\n",
    "- An episode is one iteration (in gymnasium, it is one execution of step)\n",
    "\n",
    "Before training the DQN, I am going to be setting up callbacks. Callbacks are a technique used in ML model training (in this case, stable-baselines3 has functions to set up callbacks) to help handle training more easily without directly changing the core algorithm. I am going to be using callbacks specifically for:\n",
    "\n",
    "- Performance monitoring\n",
    "- Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9343181",
   "metadata": {},
   "source": [
    "### Setting up Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f42a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74c29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the environment to see if it complies with Gymnasium's requirements\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05432bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a0af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './runs/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1272f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up callback to occur every 1000 steps and save it to the designated directory\n",
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c01dd",
   "metadata": {},
   "source": [
    "### Build/Train DQN\n",
    "\n",
    "After researching a little bit, DQN is essentially a neural network that approximates a Q-value function (this is the function used to calculate the reward for the agent based on its action within a state). It amplifies the computing power of Q-learning, which is limited to a table.\n",
    "After understanding DQN on a conceptual level, I explored how it is actually implemented in PyTorch. Since I am working with an RL model, I need to pass a policy to the DQN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f02e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DQN algorithm from stable_baselines3 library\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1db42f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(Env):\n",
    "    # set up environment and observation shapes\n",
    "    def __init__(self):\n",
    "        # subclass the model\n",
    "        super().__init__()\n",
    "        # set up spaces (gymnasium containers to represent an observation and actions)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,83,150), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3) # define 3 unique actions agent can take\n",
    "        # set up parameters for game extracting an observation of the game using mss\n",
    "        self.cap = mss()\n",
    "        # defining observation of environment window size (preprocessing)\n",
    "        self.game_location = {'top':150, 'left':80, 'width':600, 'height':200}\n",
    "        self.done_location = {'top':185, 'left':330, 'width':300, 'height':70}\n",
    "\n",
    "        # bookkeeping for reward shaping\n",
    "        self.step_count = 0\n",
    "        self.next_bonus_step = 60\n",
    "        self.bonus_interval = 20\n",
    "        self.jump_penalty = 0.02\n",
    "        self.alive_reward = 0.01\n",
    "\n",
    "    # simulate agent observation, action, and environment response\n",
    "    def step(self, action):\n",
    "        # agent has 3 actions - 0 = jump (space), 1 = crouch (down key), 2 = nothing\n",
    "        action_map = {\n",
    "            0: 'space',\n",
    "            2: 'no_op'\n",
    "        }\n",
    "        # use pydirectinput to press key related to desired action\n",
    "        if action == 0:\n",
    "                pydirectinput.keyDown('space')\n",
    "                pydirectinput.keyUp('space')\n",
    "        # action == 2 does nothing (no-op)\n",
    "\n",
    "        #increment step counter\n",
    "        self.step_count += 1\n",
    "        # check if game is done/environment is terminated\n",
    "        done, done_cap = self.get_done()\n",
    "        # penalize agent if done is true\n",
    "        if done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = self.alive_reward\n",
    "\n",
    "        # reward agent if it clears the first obstacle (around time step 60)\n",
    "        if not done and self.step_count == self.next_bonus_step:\n",
    "            reward += 2\n",
    "            self.next_bonus_step += self.bonus_interval\n",
    "\n",
    "        # penalize agent for jumping (discourage spamming)\n",
    "        if action == 0 :\n",
    "            reward -= self.jump_penalty\n",
    "        # check if game is done IF TIME CONSTRAINT IS ADDED TO EPISODE\n",
    "        truncated = False\n",
    "        # get the next observation of the environment to return to the agent\n",
    "        next_observation = self.get_observation()\n",
    "        info = {} # can leave info empty, not entirely necessary for this project\n",
    "        return next_observation, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    # visualize the game (not really used since I implement game capture via mss)\n",
    "    def render(self):\n",
    "        cv2.imshow('Game,', np.array(self.cap.grab(self.game_location))[:,:,:3])\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "\n",
    "    # ends an observation\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # reset the game environment (can follow environment termination)\n",
    "    def reset(self, *, seed=None, option=None):\n",
    "        # set seed for reproducibility of the reset\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        #set buffer between transitions\n",
    "        # time.sleep(1.5)\n",
    "        #move mouse to top left of screen and click to restart game\n",
    "        pydirectinput.click(x=300, y=300)\n",
    "        pydirectinput.press('space')\n",
    "        \n",
    "        # Gymnasium requires an observation and info to be returned\n",
    "        info = {}\n",
    "        return self.get_observation(), info\n",
    "\n",
    "    # extra methods not required to build Gymnasium environment\n",
    "\n",
    "    # grab observation of the game environment\n",
    "    def get_observation(self):\n",
    "        # grab raw screen capture of the game_location using mss and turning it into an array\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        # preprocessing\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (150,83))\n",
    "        channel = np.reshape(resized, (1,83,150))\n",
    "        return channel\n",
    "    # extract game over text with OCR via pytesseract\n",
    "    def get_done(self):\n",
    "        done = False\n",
    "        # grab raw screen capture of the done_location using mss\n",
    "        done_cap = np.array(self.cap.grab(self.done_location))[:,:,:3]\n",
    "        # coordinates for checking if a certain pixel of game over exists\n",
    "        check_x, check_y = 38, 200\n",
    "        pixel_color = done_cap[check_x, check_y]\n",
    "        # specific rgb value that is expected to appear when game is over\n",
    "        game_over_rgb = np.array([172,172,172])\n",
    "        tolerance = 10\n",
    "        if np.all(np.abs(pixel_color - game_over_rgb) <= tolerance):\n",
    "            done = True\n",
    "        \n",
    "        return done, done_cap\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "888485f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e1c9043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, buffer_size=100000, learning_starts=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9243938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/DQN_38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 51.8     |\n",
      "|    ep_rew_mean      | -0.772   |\n",
      "|    exploration_rate | 0.803    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 207      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0296   |\n",
      "|    n_updates        | 51       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 63.6     |\n",
      "|    ep_rew_mean      | -0.689   |\n",
      "|    exploration_rate | 0.516    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 509      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000307 |\n",
      "|    n_updates        | 127      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 80.4     |\n",
      "|    ep_rew_mean      | -0.494   |\n",
      "|    exploration_rate | 0.0832   |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 15       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 965      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000393 |\n",
      "|    n_updates        | 241      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.7     |\n",
      "|    ep_rew_mean      | -0.369   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 16       |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 1467     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000414 |\n",
      "|    n_updates        | 366      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.2     |\n",
      "|    ep_rew_mean      | -0.293   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 1965     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000347 |\n",
      "|    n_updates        | 491      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -0.217   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 19       |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 2525     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000478 |\n",
      "|    n_updates        | 631      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 109      |\n",
      "|    ep_rew_mean      | -0.168   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 19       |\n",
      "|    time_elapsed     | 153      |\n",
      "|    total_timesteps  | 3049     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000556 |\n",
      "|    n_updates        | 762      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 109      |\n",
      "|    ep_rew_mean      | -0.167   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 19       |\n",
      "|    time_elapsed     | 173      |\n",
      "|    total_timesteps  | 3474     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000115 |\n",
      "|    n_updates        | 868      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 110      |\n",
      "|    ep_rew_mean      | -0.142   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 194      |\n",
      "|    total_timesteps  | 3970     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000183 |\n",
      "|    n_updates        | 992      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 113      |\n",
      "|    ep_rew_mean      | -0.112   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 215      |\n",
      "|    total_timesteps  | 4509     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.76e-05 |\n",
      "|    n_updates        | 1127     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 117      |\n",
      "|    ep_rew_mean      | -0.0564  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 238      |\n",
      "|    total_timesteps  | 5166     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.98e-05 |\n",
      "|    n_updates        | 1291     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 121      |\n",
      "|    ep_rew_mean      | -0.0148  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 259      |\n",
      "|    total_timesteps  | 5797     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000413 |\n",
      "|    n_updates        | 1449     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 124      |\n",
      "|    ep_rew_mean      | 0.0146   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 283      |\n",
      "|    total_timesteps  | 6428     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.74e-05 |\n",
      "|    n_updates        | 1606     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 127      |\n",
      "|    ep_rew_mean      | 0.0557   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 307      |\n",
      "|    total_timesteps  | 7126     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000272 |\n",
      "|    n_updates        | 1781     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 130      |\n",
      "|    ep_rew_mean      | 0.0863   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 329      |\n",
      "|    total_timesteps  | 7782     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.74e-05 |\n",
      "|    n_updates        | 1945     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 132      |\n",
      "|    ep_rew_mean      | 0.114    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 24       |\n",
      "|    time_elapsed     | 350      |\n",
      "|    total_timesteps  | 8440     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00019  |\n",
      "|    n_updates        | 2109     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 133      |\n",
      "|    ep_rew_mean      | 0.134    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 24       |\n",
      "|    time_elapsed     | 372      |\n",
      "|    total_timesteps  | 9077     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00301  |\n",
      "|    n_updates        | 2269     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 135      |\n",
      "|    ep_rew_mean      | 0.151    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 24       |\n",
      "|    time_elapsed     | 392      |\n",
      "|    total_timesteps  | 9693     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.99e-05 |\n",
      "|    n_updates        | 2423     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x2cb0554cb90>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "model.learn(total_timesteps=10000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fbd418",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b54712a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justf\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 12.46GB > 7.35GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = DQN.load(\"./runs/best_model_510000\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6a2963b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 0 is 1.769999999999985\n",
      "Total Reward for episode 1 is 1.629999999999988\n",
      "Total Reward for episode 2 is 1.579999999999989\n",
      "Total Reward for episode 3 is 3.1699999999999555\n",
      "Total Reward for episode 4 is 1.7499999999999853\n"
     ]
    }
   ],
   "source": [
    "for episode in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(int(action))\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "    print(f'Total Reward for episode {episode} is {total_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6205dfb1",
   "metadata": {},
   "source": [
    "## Optimizing Preprocessing\n",
    "\n",
    "My initial model's rollouts were processing 3 frames per second. This can obviously be improved, starting with optimizing preprocessing. Since the RL agent is working with vectorized image data, a lot of optimizing techniques will be centered around image processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca4bc3",
   "metadata": {},
   "source": [
    "### Profiling functions\n",
    "\n",
    "Before starting to optimize, I decided to profile the key functions of the environment to pinpoint where performance is being lost. I modified the following functions where most of the image processing occurs:\n",
    "\n",
    "1. step\n",
    "2. get_observation\n",
    "3. get_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbf1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated game environment to discover bottlenecks\n",
    "class Game(Env):\n",
    "    # set up environment and observation shapes\n",
    "    def __init__(self):\n",
    "        # subclass the model\n",
    "        super().__init__()\n",
    "        # set up spaces (gymnasium containers to represent an observation and actions)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,83,150), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3) # define 3 unique actions agent can take\n",
    "        # set up parameters for game extracting an observation of the game using mss\n",
    "        self.cap = mss()\n",
    "        # defining observation of environment window size (preprocessing)\n",
    "        self.game_location = {'top':150, 'left':80, 'width':600, 'height':200}\n",
    "        self.done_location = {'top':185, 'left':330, 'width':300, 'height':70}\n",
    "\n",
    "    # simulate agent observation, action, and environment response\n",
    "    def step(self, action):\n",
    "        start_time = time.time()\n",
    "        action_map = {\n",
    "            0: 'space',\n",
    "            1: 'down',\n",
    "            2: 'no_op'\n",
    "        }\n",
    "        # 1. Action input\n",
    "        t0 = time.time()\n",
    "        if action == 0:\n",
    "            pydirectinput.keyDown('space')\n",
    "            pydirectinput.keyUp('space')\n",
    "        elif action == 1:\n",
    "            pydirectinput.keyDown('down')\n",
    "            pydirectinput.keyUp('down')\n",
    "        t1 = time.time()\n",
    "\n",
    "        # 2. Check done condition\n",
    "        done_check_start = time.time()\n",
    "        done, _ = self.get_done()\n",
    "        done_check_end = time.time()\n",
    "\n",
    "        # 3. Get next observation\n",
    "        obs_start = time.time()\n",
    "        next_observation = self.get_observation()\n",
    "        obs_end = time.time()\n",
    "\n",
    "        # 4. Pack outputs\n",
    "        reward = 1\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        # 5. Report timings\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n[STEP PROFILE]\")\n",
    "        print(f\"Action input: {t1 - t0:.4f}s\")\n",
    "        print(f\"Check done : {done_check_end - done_check_start:.4f}s\")\n",
    "        print(f\"Get obs    : {obs_end - obs_start:.4f}s\")\n",
    "        print(f\"Total step : {total_time:.4f}s\\n\")\n",
    "\n",
    "        return next_observation, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    # visualize the game (not really used since I implement game capture via mss)\n",
    "    def render(self):\n",
    "        cv2.imshow('Game,', np.array(self.cap.grab(self.game_location))[:,:,:3])\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "\n",
    "    # ends an observation\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # reset the game environment (can follow environment termination)\n",
    "    def reset(self, *, seed=None, option=None):\n",
    "        # set seed for reproducibility of the reset\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        #set buffer between transitions\n",
    "        # time.sleep(1.5)\n",
    "        #move mouse to top left of screen and click to restart game\n",
    "        pydirectinput.click(x=300, y=300)\n",
    "        pydirectinput.press('space')\n",
    "        \n",
    "        # Gymnasium requires an observation and info to be returned\n",
    "        info = {}\n",
    "        return self.get_observation(), info\n",
    "\n",
    "    # extra methods not required to build Gymnasium environment\n",
    "\n",
    "    # grab observation of the game environment\n",
    "    def get_observation(self):\n",
    "        start = time.time()\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        grab_time = time.time()\n",
    "\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (150,83))\n",
    "        channel = np.reshape(resize, (1,83,150))\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"[GET_OBS] Grab: {grab_time - start:.4f}s | Preprocess: {end - grab_time:.4f}s\")\n",
    "        return channel\n",
    "\n",
    "    def get_done(self):\n",
    "        start = time.time()\n",
    "        done = False\n",
    "        done_cap = np.array(self.cap.grab(self.done_location))[:,:,:3]\n",
    "        grab_time = time.time()\n",
    "\n",
    "        pixel_color = done_cap[38, 200]\n",
    "        game_over_rgb = np.array([172,172,172])\n",
    "        tolerance = 10\n",
    "        if np.all(np.abs(pixel_color - game_over_rgb) <= tolerance):\n",
    "            done = True\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"[GET_DONE] Grab: {grab_time - start:.4f}s | Pixel check: {end - grab_time:.4f}s\")\n",
    "        return done, done_cap   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28d4e1",
   "metadata": {},
   "source": [
    "### Profiling Results\n",
    "\n",
    "#### pydirectinput optimization\n",
    "\n",
    "After running a couple of episodes with the updated functions, I lose a lot of time because of pydirectinput.press(). The output below shows more detail:\n",
    "\n",
    "```\n",
    "[GET_DONE] Grab: 0.0089s | Pixel check: 0.0001s\n",
    "[GET_OBS] Grab: 0.0074s | Preprocess: 0.0007s\n",
    "\n",
    "[STEP PROFILE]\n",
    "Action input: 0.3017s\n",
    "Check done : 0.0091s\n",
    "Get obs    : 0.0082s\n",
    "Total step : 0.3190s\n",
    "```\n",
    "\n",
    "The method has a lot of internal delay, so instead I can replace the actions with keyDown() and keyUp().\n",
    "\n",
    "The updated step will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, action):\n",
    "    # agent has 3 actions - 0 = jump (space), 1 = crouch (down key), 2 = nothing\n",
    "    action_map = {\n",
    "        0: 'space',\n",
    "        1: 'down',\n",
    "        2: 'no_op'\n",
    "    }\n",
    "    # use pydirectinput to press key related to desired action\n",
    "    if action == 0:\n",
    "            pydirectinput.keyDown('space')\n",
    "            pydirectinput.keyUp('space')\n",
    "    elif action == 1:\n",
    "        pydirectinput.keyDown('down')\n",
    "        pydirectinput.keyUp('down')\n",
    "    # action == 2 does nothing (no-op)\n",
    "    # Gymnasium defines 4 outputs for step: next_observation, reward, terminate, info\n",
    "\n",
    "    # check if game is done/environment is terminated\n",
    "    done, done_cap = self.get_done()\n",
    "    # check if game is done IF TIME CONSTRAINT IS ADDED TO EPISODE\n",
    "    truncated = False\n",
    "    # get the next observation of the environment to return to the agent\n",
    "    next_observation = self.get_observation()\n",
    "    # give the agent a reward of 1 for each frame it is alive\n",
    "    reward = 1 \n",
    "    info = {} # can leave info empty, not entirely necessary for this project\n",
    "    return next_observation, reward, done, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Game()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
