{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9412fbd8",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba09338",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "!pip3 install pydirectinput\n",
    "!pip3 install stable-baselines3[extra] protobuf==3.20.*\n",
    "!pip3 install mss\n",
    "!pip3 install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98ce851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import pydirectinput\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import keyboard\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda60868",
   "metadata": {},
   "source": [
    "# Build environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf28f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(Env):\n",
    "    # set up environment and observation shapes\n",
    "    def __init__(self):\n",
    "        # subclass the model\n",
    "        super().__init__()\n",
    "        # set up spaces (gymnasium containers to represent an observation and actions)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,83,150), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3) # define 3 unique actions agent can take\n",
    "        # set up parameters for game extracting an observation of the game using mss\n",
    "        self.cap = mss()\n",
    "        # defining observation of environment window size (preprocessing)\n",
    "        self.game_location = {'top':150, 'left':80, 'width':600, 'height':200}\n",
    "        self.done_location = {'top':185, 'left':330, 'width':300, 'height':70}\n",
    "\n",
    "        # bookkeeping for reward shaping\n",
    "        self.step_count = 0\n",
    "        self.next_bonus_step = 60\n",
    "        self.bonus_interval = 20\n",
    "        self.jump_penalty = 0.02\n",
    "        self.alive_reward = 0.01\n",
    "\n",
    "    # simulate agent observation, action, and environment response\n",
    "    def step(self, action):\n",
    "        # agent has 3 actions - 0 = jump (space), 1 = crouch (down key), 2 = nothing\n",
    "        action_map = {\n",
    "            0: 'space',\n",
    "            2: 'no_op'\n",
    "        }\n",
    "        # use pydirectinput to press key related to desired action\n",
    "        if action == 0:\n",
    "                pydirectinput.keyDown('space')\n",
    "                pydirectinput.keyUp('space')\n",
    "        # action == 2 does nothing (no-op)\n",
    "\n",
    "        #increment step counter\n",
    "        self.step_count += 1\n",
    "        # check if game is done/environment is terminated\n",
    "        done, done_cap = self.get_done()\n",
    "        # penalize agent if done is true\n",
    "        if done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = self.alive_reward\n",
    "\n",
    "        # reward agent if it clears the first obstacle (around time step 60)\n",
    "        if not done and self.step_count == self.next_bonus_step:\n",
    "            reward += 2\n",
    "            self.next_bonus_step += self.bonus_interval\n",
    "\n",
    "        # penalize agent for jumping (discourage spamming)\n",
    "        if action == 0 :\n",
    "            reward -= self.jump_penalty\n",
    "        # check if game is done IF TIME CONSTRAINT IS ADDED TO EPISODE\n",
    "        truncated = False\n",
    "        # get the next observation of the environment to return to the agent\n",
    "        next_observation = self.get_observation()\n",
    "        info = {} # can leave info empty, not entirely necessary for this project\n",
    "        return next_observation, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    # visualize the game (not really used since I implement game capture via mss)\n",
    "    def render(self):\n",
    "        cv2.imshow('Game,', np.array(self.cap.grab(self.game_location))[:,:,:3])\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "\n",
    "    # ends an observation\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # reset the game environment (can follow environment termination)\n",
    "    def reset(self, *, seed=None, option=None):\n",
    "        # set seed for reproducibility of the reset\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        #set buffer between transitions\n",
    "        # time.sleep(1.5)\n",
    "        #move mouse to top left of screen and click to restart game\n",
    "        pydirectinput.click(x=300, y=300)\n",
    "        pydirectinput.press('space')\n",
    "        \n",
    "        # Gymnasium requires an observation and info to be returned\n",
    "        info = {}\n",
    "        return self.get_observation(), info\n",
    "\n",
    "    # extra methods not required to build Gymnasium environment\n",
    "\n",
    "    # grab observation of the game environment\n",
    "    def get_observation(self):\n",
    "        # grab raw screen capture of the game_location using mss and turning it into an array\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        # preprocessing\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (150,83))\n",
    "        channel = np.reshape(resized, (1,83,150))\n",
    "        return channel\n",
    "    # extract game over text with OCR via pytesseract\n",
    "    def get_done(self):\n",
    "        done = False\n",
    "        # grab raw screen capture of the done_location using mss\n",
    "        done_cap = np.array(self.cap.grab(self.done_location))[:,:,:3]\n",
    "        # coordinates for checking if a certain pixel of game over exists\n",
    "        check_x, check_y = 38, 200\n",
    "        pixel_color = done_cap[check_x, check_y]\n",
    "        # specific rgb value that is expected to appear when game is over\n",
    "        game_over_rgb = np.array([172,172,172])\n",
    "        tolerance = 10\n",
    "        if np.all(np.abs(pixel_color - game_over_rgb) <= tolerance):\n",
    "            done = True\n",
    "        \n",
    "        return done, done_cap\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12a276b",
   "metadata": {},
   "source": [
    "# Set up callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './runs/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca18dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up callback to occur every 1000 steps and save it to the designated directory\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf0b687",
   "metadata": {},
   "source": [
    "# Build/Train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2bc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca52922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = DQN('CnnPolicy', env, learning_rate=0.0001, tensorboard_log=LOG_DIR, verbose=1, buffer_size=300000, learning_starts=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "model.learn(total_timesteps=50000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c9301",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec59d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = DQN.load(os.path.join('runs', 'learning-rate-0001'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(int(action))\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "    print(f'Total Reward for episode {episode} is {total_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb57297",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d9fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rewards_from_event_file(path):\n",
    "    ea = EventAccumulator(path)\n",
    "    ea.Reload()\n",
    "\n",
    "    if 'rollout/ep_rew_mean' not in ea.Tags()['scalars']:\n",
    "        return None\n",
    "\n",
    "    scalar_events = ea.Scalars('rollout/ep_rew_mean')\n",
    "    steps = [e.step for e in scalar_events]\n",
    "    values = [e.value for e in scalar_events]\n",
    "\n",
    "    return steps, values\n",
    "\n",
    "def plot_tensorboard_rewards(logs_dir):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for folder in sorted(os.listdir(logs_dir)):\n",
    "        folder_path = os.path.join(logs_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        event_files = [f for f in os.listdir(folder_path) if f.startswith(\"events.out\")]\n",
    "        if not event_files:\n",
    "            continue\n",
    "\n",
    "        steps, rewards = None, None\n",
    "        for f in event_files:\n",
    "            event_path = os.path.join(folder_path, f)\n",
    "            result = extract_rewards_from_event_file(event_path)\n",
    "            if result:\n",
    "                steps, rewards = result\n",
    "                break\n",
    "\n",
    "        if steps and rewards:\n",
    "            plt.plot(steps, rewards, label=folder)\n",
    "\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Mean Episode Reward\")\n",
    "    plt.title(\"Training Reward Progress (from TensorBoard logs)\")\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1.0), fontsize='small')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_tensorboard_rewards('./logs')  # replace with your actual log directory\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
